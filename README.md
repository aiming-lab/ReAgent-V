# ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding

[Yiyang Zhou*](https://yiyangzhou.github.io/),   [Yangfan He*](https://github.com/codepassionor),   [Yaofeng Su](https://github.com/InfiniteLoopCoder),   [Siwei Han](https://github.com/Lillianwei-h),   [Joel Jang](https://github.com/joeljang), [Gedas Bertasius](https://www.gedasbertasius.com/),   [Mohit Bansal](https://www.cs.unc.edu/~mbansal/),   [Huaxiu Yao](https://www.huaxiuyao.io/)
<div align="center">
*Equal Contribution
</div>
<div align="center">
    <a href="https://arxiv.org/abs/2506.01300"><img src="assets/Paper-Arxiv-orange.svg" ></a>
</div>

**ReAgent-V** is a **modular**, **extensible**, and **reward-aware** video reasoning framework designed to elevate video question answering and reasoning through:

- ğŸ”§ **Flexible Tool Integration** â€” Plug-and-play support for OCR, ASR, object detection, scene graph generation, captioning, and more  
- ğŸ§  **Reward-Guided Inference** â€” Enables real-time self-correction via structured reward signals  
- ğŸ¯ **Adaptive Model Alignment** â€” Aligns models dynamically based on inference-time feedback  
- ğŸ—‚ï¸ **High-Quality Data Selection** â€” Facilitates sample-efficient learning using reflective evaluation  
- ğŸ“Š **Entropy-Calibrated Frame Selection** â€” Prioritizes key frames for focused reasoning  
- ğŸ” **Multi-Perspective Reflection** â€” Refines answers through debate among conservative, neutral, and aggressive viewpoints

  
## News
* ğŸ”¥ [10.03] Our paper is online now: https://arxiv.org/abs/2506.01300.


---

# ğŸ“Œ Overview
![Framework Overview](assets/framework.png)  
### ReAgent-V consists of two major components:

## ğŸ¥ Video Understanding 

- **Entropy-Calibrated Frame Selection**  
  Efficiently selects the most informative frames for video reasoning.

- **Tool-Augmented Inference**  
  Dynamically integrates multimodal tools including OCR, ASR, object detection, scene graph generation, and captioning.

- **Multi-Agent Reflection**  
  Iteratively refines outputs by encouraging disagreement and consensus among diverse agent personas (conservative / neutral / aggressive).

- ğŸ“ Module: `ReAgent-V`  
- ğŸ“˜ Instructions: [Video Understanding README](https://github.com/aiming-lab/ReAgent-V/blob/main/ReAgent-V/README.md)

---

## ğŸš€ Applications

ReAgent-V supports a range of real-world tasks via dedicated application modules:

### ğŸ§­ VLA Alignment  
Aligns **Vision-Language-Action (VLA)** models using **Trajectory-wise Preference Optimization (TPO)** guided by ReAgent-Vâ€™s reward feedback.

- ğŸ“ Module: `Application/VLA-Alignment`  
- ğŸ“˜ Instructions: [VLA Alignment README](https://github.com/aiming-lab/ReAgent-V/blob/main/Application/VLA-Alignment/README.md)

### ğŸ” Video LLM Reasoning *(Coming Soon)*  
Supports reflection-based evaluation and reward-guided sample curation for long-form video LLM fine-tuning using **GRPO** strategies.

- ğŸ“ Module: `Application/LLM-Reasoning`  

---

## ğŸ§‘â€ğŸ’» Getting Started

Each subfolder contains its own `README.md` with detailed installation, setup, and training instructions. To get started:

1. Clone the repository  
2. Follow the environment setup and requirements in each module  
3. Explore the demo scripts and customize as needed

ğŸ’¬ If you have questions or encounter any issues, feel free to open an [issue](https://github.com/aiming-lab/ReAgent-V/issues) or contact the maintainers.

---

## ğŸ“š Citation

If you find ReAgent-V helpful in your research or projects, please consider citing:

```bibtex
@article{zhou2025reagent,
  title={ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding},
  author={Zhou, Yiyang and He, Yangfan and Su, Yaofeng and Han, Siwei and Jang, Joel and Bertasius, Gedas and Bansal, Mohit and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2506.01300},
  year={2025}
}
