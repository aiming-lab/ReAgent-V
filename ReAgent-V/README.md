**ReAgent-V** is a flexible and modular video understanding framework that goes far beyond traditional Video-QA systems. It seamlessly integrates:

- 🔍 **Entropy-calibrated frame selection**
- 🛠️ **Tool-augmented reasoning with dynamic tool invocation**
- 🪞 **Multi-perspective reflection strategies** (conservative, neutral, aggressive)
- 🧾 **Real-time, inference-aware reward generation**

---

### 🔧 Modular Tool Integration

**ReAgent-V** features a **modular, plug-and-play tool architecture**, enabling **dynamic and adaptive integration** of a wide spectrum of multimodal tools. This architecture empowers the system to flexibly invoke the most suitable tools based on task-specific needs and contextual cues. Supported tools include:

* 📝 **Optical Character Recognition (OCR)**
* 🔊 **Automatic Speech Recognition (ASR)**
* 🧭 **Object Detection (DET)**
* 🕸️ **Scene Graph Generation**
* 🖼️ **Image and Video Captioning**
* 🔗 **Multimodal Matching Models** (e.g., CLIP)
* 🧠 **Advanced Video Reasoning Modules**

Each tool is seamlessly integrated through **customized, context-aware prompt templates**, ensuring high precision, robustness, and task adaptability across diverse video understanding scenarios.

---

### 🧠 Reflective Agentic Reasoning

**ReAgent-V** incorporates a powerful **Critic Agent** to perform **real-time answer evaluation and structured feedback generation**. This feedback-driven mechanism enables iterative refinement through three distinct reflective strategies:

* 🛡️ **Conservative Revision**: Minimal adjustments focused solely on output fluency and clarity
* ⚖️ **Neutral Refinement**: Entity-level verification and correction to ensure factual consistency
* 🔍 **Aggressive Reasoning Revision**: Comprehensive updates involving both entity accuracy and logical coherence

By **aggregating multi-perspective reflections**, ReAgent-V synthesizes a **robust and contextually grounded final answer**, improving reliability and interpretability in complex video reasoning tasks.

---

### 🏆 Self-Improving via Reward-Based Feedback

The **evaluation reports** generated by ReAgent-V fulfill **dual roles** in the system pipeline:

* 🎯 **Real-Time Inference Refinement**: Providing actionable feedback to guide dynamic answer revision during inference
* 📚 **Training Signal Selection**: Extracting high-quality samples for supervision using **SFT**, **DPO**, and **GRPO** strategies

This architecture forms a **data-centric feedback loop**, fostering **continual self-improvement** through alignment between inference-time critique and training-time supervision.
Crucially, it unifies **training and inference** via a shared evaluation backbone, enabling the system to **learn from its own reasoning behavior** and evolve over time.

---

## 🌐 Project Structure

```text
ReAgentV-NeXT/                             # Repository root
├── ReAgentV.py                           # Core engine: orchestrates multimodal retrieval, prompt synthesis, and QA flow
├── run_pipeline.py                       # End-to-end demo: shows how to run the complete Video-QA pipeline
├── ReAgentV_utils/                       # All utility modules and helper scripts
│   ├── critical_question_generator/      # “Critical Question” generation logic
│   │   └── generate_critical_question.py # Code to produce follow-up critical questions
│   ├── frame_selection_ecrs/             # Entropy-Calibrated Frame Selection (ECRS) module
│   │   └── ECRS_frame_selection.py       # ECRS implementation for optimal key-frame extraction
│   ├── model_inference/                  # Inference wrappers for LLaVA and related models
│   │   └── model_inference.py            # High-performance inference routines
│   ├── model_loader/                     # Model initialization and config loader
│   │   ├── load_config_vars.py           # Reads global YAML configuration
│   │   └── load_default.py               # Launches CLIP, Whisper, LLaVA, etc., with best-practice defaults
│   ├── prompt_builder/                   # Multimodal prompt assembly logic
│   │   ├── build_multimodal_prompt.py    # Crafts the ultimate prompt from multimodal inputs
│   │   └── prompt.py                     # Sophisticated prompt templates and formatting
│   ├── ReAgentV_config/                  # Centralized configuration folder
│   │   └── config.yaml                   # All thresholds, flags, and hyperparameters in one place
│   └── tools/                            # Full suite of multimodal tools (OCR, ASR, Object Detection, etc.)
│       ├── audio_tools/                  # Audio processing + advanced ASR
│       │   └── asr_utils.py              # Speech-to-text implementation
│       ├── ocr_tools/                    # OCR extraction and text processing
│       │   └── ocr_utils.py              # OCR implementation
│       ├── rag_retriever_dynamic.py      # Dynamic RAG (Retrieval-Augmented Generation) helper
│       ├── scene_graph_tools/            # Scene Graph analysis & object detection
│       │   ├── det_utils.py              # Object detection models + parsing utilities
│       │   ├── filter_keywords.py        # Keyword extraction for guiding scene graph queries
│       │   └── scene_graph.py            # Scene graph construction and relationship summarization
│       ├── tool_selection.py             # Intelligent tool-selection logic per query
│       └── video_processor/              # Video and audio preprocessing pipelines
│           ├── process_audio.py          # High-fidelity audio extraction and pre-processing
│           └── process_video.py          # ffmpeg-based frame sampling, resizing, and normalization
├── models/                               # Pretrained model weights (CLIP, Whisper, LLaVA) and caches
│   ├── clip-vit-large-patch14-336/       # CLIP weight snapshots
│   │   └── snapshots/<commit-id>/        # Specific snapshot ID
│   ├── whisper-large/                    # Whisper weight snapshots
│   │   └── snapshots/<commit-id>/
│   └── llava-video-7b-qwen2/             # LLaVA-Video-7B-Qwen2 weight snapshots
│       └── snapshots/<commit-id>/
├── requirements.txt                      # Python dependencies (for `pip install -r requirements.txt`)
└── README.md                             # This file: world-class documentation
````

---

## ⚡ Quickstart: Instant Expert Setup

1. **Clone the Repository**

   ```bash
   git clone https://github.com/aiming-lab/ReAgent-V.git
   cd ReAgent-V
   ```

2. **Create and Activate a Conda Environment (Optional but Recommended)**

   ```bash
   conda create -n reagenv_env python=3.9 -y
   conda activate reagenv_env
   ```

3. **Install Required Python Packages**

   ```bash
   pip install -r requirements.txt
   ```

   > We’ve curated a lean, battle-tested `requirements.txt`—no fluff, no conflicts. Just world-class AI libraries.

4. **Download Pretrained Weights**

   * **CLIP (ViT-L/14-336)**
     Download from: `https://huggingface.co/openai/clip-vit-large-patch14-336`
     Unpack under:

     ```text
     models/clip-vit-large-patch14-336/snapshots/<commit-id>/
     ```

   * **Whisper (Large)**
     Download from: `https://huggingface.co/openai/whisper-large`
     Unpack under:

     ```text
     models/whisper-large/snapshots/<commit-id>/
     ```

   * **LLaVA-Video-7B-Qwen2**
     Download from: `https://huggingface.co/lmms-lab/LLaVA-Video-7B-Qwen2`
     Unpack under:

     ```text
     models/llava-video-7b-qwen2/snapshots/<commit-id>/
     ```

   > Use the correct `<commit-id>` for reproducibility. Top experts track commits meticulously; so do we.

5. **Configure `path_dict` in `run_pipeline.py`**

   ```python
   path_dict = {
       "clip_model_path"   : "models/clip-vit-large-patch14-336/snapshots/ce19dc912ca5cd21c8a653c79e251e808ccabcd1",
       "clip_cache_dir"    : "models/clip-vit-large-patch14-336",
       "whisper_model_path": "models/whisper-large/snapshots/4ef9b41f0d4fe232daafdb5f76bb1dd8b23e01d7",
       "whisper_cache_dir" : "models/whisper-large",
       "llava_model_path"  : "models/llava-video-7b-qwen2/snapshots/013210b3aff822f1558b166d39c1046dd109520f",
       "llava_cache_dir"   : "models/llava-video-7b-qwen2",
   }
   ```

6. ## 🚀 Run the Pipeline

```bash
python run_pipeline.py
```

At this point, you’ll witness ReAgent-V in action: **lightning-fast frame extraction**, **precise multimodal retrieval**, and **reflective answer generation**—all orchestrated in a modular, extensible fashion.

---

## 🎬 Example: `run_pipeline.py` Walkthrough

ReAgent-V’s inference script demonstrates a fully modular and highly adaptable architecture. Below is a breakdown of its flexible components:

---

### 🔗 1. Load Models — *Plug-and-Play VLLMs*

```python
qa_system = ReAgentV.load_default(path_dict)
```

* Easily switch between LLaVA, Qwen, Vicuna, or any VLLM model.
* Paths are defined in `path_dict` and support HuggingFace snapshot-style loading.

---

### 🎞️ 2. Frame Sampling — *ECRS Based Selection*

```python
frames, key_frames, ... = qa_system.load_and_sample_video(question, video_path)
```

* Sampling adapts to the question (e.g., relevance-based, entropy-based).
* Supports customization of sampling strategy and max frame count.

---

### 🛠️ 3. Visual Tool Invocation — *Extensive and Dynamic Tool Use*

```python
modal_info, ..., USE_OCR, USE_ASR, USE_DET = qa_system.retrieve_modal_info(...)
```

ReAgent-V is designed with extensibility in mind:
**Adding new tools is as simple as listing the models and defining when & how they should be used.**

**Workflow:**

1. **List available models** for tool use (e.g., OCR, ASR, Scene Graph, Captioning, etc.)
2. **Dynamically select tools** at runtime based on the input question or system prompt
3. **Load tool-specific models** only when selected (lazy loading supported)
4. **Call tool modules** to extract relevant information (text, objects, audio, graphs...)
5. **Inject results into the multimodal prompt** via template composition

```python
# Example: dynamically select and use **part of the mentioned tools**
if "OCR" in selected_tools:
    ocr_result = run_ocr_model(...)
if "ASR" in selected_tools:
    asr_result = run_asr_model(...)
if "Scene Graph" in selected_tools:
    graph_result = run_scene_graph_model(...)

# Build the prompt with available tool outputs
prompt = build_multimodal_prompt(
    question=question,
    ocr=ocr_result,
    asr=asr_result,
    scene_graph=graph_result,
    ...
)
```

> ✅ This modular design makes it easy to plug in **any vision or language model**, route decisions through LLMs, and construct **task-aware prompts** using only the necessary tools.

---

### 🧩 4. Prompt Construction — *Compositional Prompt Templates*

```python
qs = qa_system.build_multimodal_prompt(...)
```

* Combines the **original question** with outputs from multiple sources:

  * 📝 OCR-extracted text
  * 🔊 ASR transcripts
  * 🧱 Object Detection and Scene Graph outputs
  * 🖼️ Other visual tools (e.g., captioning, layout analysis, retrieval cues)

* Prompt templates are fully **modular** and can be tailored to diverse tasks, including:

  * 🧠 Reasoning and inference
  * 🎬 Video captioning and narration
  * ❓ Multimodal question answering
  * 🎯 Reward modeling and training signal generation

> 🔄 Only relevant tools are incorporated based on task and context, enabling **minimal yet informative** prompt construction for efficient inference.

---

### 🧠 5. Inference — *Flexible VLLM Integration* (using `llava_model` as an example)

```python
initial_answer = llava_inference(qs, video_tensor)
```

* Performs inference using a **VLLM backend**, such as `llava_model`
* Accepts the **composed multimodal prompt** and corresponding **video tensor embedding**
* Designed for **backend flexibility** — you can easily swap in other models like **Qwen**, **Vicuna**, or any custom LLM API

> 🔄 The inference interface is fully decoupled, allowing seamless integration with local, server-based, or cloud-hosted LLMs—no need to modify upstream processing logic.


---

### 🔄 6. Tool Re-selection via Critical Questions

```python
updated_infos = {}
for cq in critique_questions:
    tool_selection_prompt = tool_retrieval_prompt_template.format(question=cq)
    response_list = llava_inference(tool_selection_prompt, video=None)

    USE_ASR = "ASR" in response_list
    USE_OCR = "OCR" in response_list
    USE_DET = "Scene Graph" in response_list

    new_modal_info, new_det_top_idx, _, _, _ = qa_system.retrieve_modal_info(...)
    updated_infos[cq] = new_modal_info
```

This block implements **tool re-selection for each critique question**, enabling *question-specific multimodal retrieval*. Here's how it works:

1. 🧠 **Language-based Tool Selection**
   Each critical question (`cq`) is inserted into a **tool-selection prompt**, which is passed to the LLM (e.g., LLaVA) to decide:

   * Do we need **OCR**?
   * Do we need **ASR**?
   * Do we need **Scene Graph**?

2. ⚙️ **Tool Re-selection & Execution**
   Based on the LLM's response (`response_list`), the pipeline:

   * Sets `USE_OCR`, `USE_ASR`, `USE_DET` flags accordingly
   * **Loads only the selected tools**
   * **Executes them** to extract updated visual/audio/textual signals for the current question

3. 🧩 **Multimodal Context Update**
   The newly extracted tool outputs (`new_modal_info`) are stored in `updated_infos[cq]` for later reflection and evaluation.

```python
context_infos = {question: modal_info, **updated_infos}
```

> ✅ This design allows **fine-grained tool scheduling per question**, driven entirely by the model’s reasoning. Each follow-up question gets **its own dynamic retrieval plan**, making the system both efficient and contextually adaptive.

---

### 📋 7. Evaluation Report — *Structured Feedback as Reward*

```python
eval_report = qa_system.generate_eval_report(...)
```

* Generates a **structured critique** based on:

  * The original question
  * Initial model answer
  * Multimodal context from both the main and critical questions
* Provides a detailed summary of:

  * ✅ Answer strengths
  * ❌ Weaknesses and logical gaps
  * 🔍 Missing visual/audio evidence

> 🔁 This report **closes the reflection loop**, bridging multimodal evidence and LLM reasoning.
> 🎯 It can also serve as a **reward signal** in downstream training via **RLHF**, **DPO**, or **GRPO**, making model updates **aligned with self-critique**.

---

### 🪞 8. Reflective Answer Refinement — *Self-Correction via Multi-Perspective Feedback*

```python
final_answer = qa_system.get_reflective_final_answer(...)
```

* Takes the **initial answer**, the **evaluation report**, and the original **video context**
* Applies **multi-perspective refinement strategies** to improve factuality, coverage, and logic:

  * 🛡️ **Conservative**: Minor surface-level edits
  * ⚖️ **Neutral**: Entity-level consistency and corrections
  * 🔍 **Aggressive**: Structural rethinking with logic and evidence updates

> 🔁 This step completes the **feedback-driven refinement loop**, allowing the model to **self-correct** based on structured critique, retrieved multimodal context, and prior reasoning outputs.

---

## ✅ Flexibility Summary

| Component             | Customizable Capability                                                               |
| --------------------- | ------------------------------------------------------------------------------------- |
| **Model Loader**      | Plug in any VLLM, CLIP, ASR, or Whisper model via the `path_dict` configuration       |
| **Prompt Templates**  | Fully modular and composable for different tasks (e.g., QA, captioning, reasoning)    |
| **Tool Selection**    | Dynamically guided by LLM reasoning; override logic easily for custom workflows       |
| **Visual Tools**      | Conditionally invoked (OCR / ASR / DET / Scene Graph / etc.) based on the input query |
| **Evaluation Report** | Structured feedback usable as a reward signal in SFT, DPO, GRPO, or RLHF pipelines    |
| **Inference Backend** | Easily switch between LLaVA, Qwen, Vicuna, or custom VLLM adapters with no rewiring   |

> 🧩 ReAgent-V is designed for **plug-and-play extensibility**, enabling effortless adaptation to new tools, models, and tasks—without modifying the core pipeline logic.


---

## 🚀 Key Features

### 1. Accurate Frame Selection (ECRS)

* **Entropy-Calibrated Ranking** for picking the most informative frames.
* Extract up to hundreds of frames and automatically downselect to the top-𝑘 that matter, saving GPU memory and inference time.

### 2. Smart Multimodal Retrieval

* Combines **OCR**, **ASR**, **Scene Graph**, and other optional **RAG** retrieval in one unified pipeline.
* **Dynamic tool-switching**: ReAgentV intelligently decides which modules to activate for each query—no manual toggles required.

### 3. Flexible Prompt Builder

* Synthesizes raw OCR text, ASR transcripts, detected object labels, keyword-based scene graph relations and other optional **RAG** into a single, coherent prompt.
* Compatible with **Zero-Shot**, **Few-Shot**, or custom template prompts—fine-tune it to your exact needs.

### 4. OSS(Open-sourced Models)-Powered Inference & Reflection

* **Initial Answer**: LLaVA digests the multimodal prompt and video tensor to generate a base response.
* **Critical Questions**: Automated follow-up question generation to probe potential weaknesses in the initial answer.
* **Evaluation Report**: Structured feedback with numerical scores, categorical reasoning, and explicit “strength/weakness/missing info” fields.
* **Reflective Final Answer**: A self-correcting mechanism that reallocates attention to overlooked video segments, ensuring your final answer is both precise and comprehensive.

### 5. Modular & Extensible Design

* Plug in new tools effortlessly: image subtitle translation, action recognition, video summarization, you name it.
* Customize **`ReAgentV_config/config.yaml`** for threshold tuning, spatial/temporal parameters, and advanced flags—control every aspect like a world-class professional.

---

## ✨ Example Output

````bash
$ python run_pipeline.py
Constructed Prompt:
“Video QA: In these key frames, a robotic arm holds a folded cloth.
 Frame 2: close-up of the hand grip.
 OCR text detected: “CLEAN”.
 ASR transcript: “Grab the blue cloth”.
 Question: In the video, what is the robotic arm grabbing?”

Initial Answer:
“The robotic arm is grabbing a blue cloth.”

Evaluation Report:
```json
{
  "structured_feedback": "The initial answer 'The robotic arm is grabbing a cloth' is generally accurate based on the visible video evidence. However, it lacks temporal accuracy as there are no timestamps provided to confirm the exact moment of the action. Option disambiguation could be improved by specifying which cloth the robotic arm is grabbing, as there are multiple cloths in the scene. Reasoning specificity is adequate as the answer directly addresses the question. Linguistic precision is acceptable, but the answer could be more detailed by describing the specific cloth or its location.",
  "scores": {
    "visual_alignment": { "value": 4.0, "reason": "The answer aligns well with the visible video evidence." },
    "temporal_accuracy": { "value": 2.0, "reason": "No timestamps are provided to confirm the exact moment of the action." },
    "option_disambiguation": { "value": 3.0, "reason": "The answer does not specify which cloth is being grabbed." },
    "reasoning_specificity": { "value": 4.0, "reason": "The answer directly addresses the question." },
    "linguistic_precision": { "value": 3.5, "reason": "The answer is grammatically correct and semantically accurate, but could be more detailed." }
  },
  "total_score": 16.5,
  "scalar_reward": 0.8
}
````

Final Answer:

```json
{
  "final_answer": "The kitchen sink is equipped with a modern faucet, a dish rack, and various cleaning supplies. A robotic arm is positioned above the sink, suggesting an automated or assistive cleaning system. The surrounding area includes a countertop with a soap dispenser and a towel."
}
```

> Notice how ReAgentV extracts, analyzes, and refines—delivering not just an answer, but a **fully annotated reasoning journey** from raw video to final verdict.

---

## 🔧 Usage Tips & Pro-Tips

* **Maximize GPU Efficiency**
  Adjust `max_frames_num` in `load_and_sample_video` to fit your VRAM. Combine frames or downscale as needed.

* **Customize Prompts**
  Edit templates inside `prompt_builder/prompt.py` to align with your domain vocabulary—medical, industrial, retail, etc.

* **Extend Tools**
  Easily drop in new modules—like caption translation, action recognition, or advanced video summarization—by adding code under `tools/` and updating `retrieve_modal_info`.

### ⚙️ **Tune Config Values**

Open `ReAgentV_config/config.yaml` to customize core settings, including:

* 🎯 Detection thresholds
* 📖 RAG passage counts
* 🌀 Beam sizes for decoding
* 🧠 Tool usage preferences (e.g., enable/disable OCR, ASR, Scene Graph)

> 🛠️ Tool activation is currently governed by **question semantics** and explicitly tied to modules like **OCR**, **ASR**, and **Scene Graph**.
> 🧩 Want to add more visual tools (e.g., **captioning**, **layout analysis**, **object tracking**)?
> Simply **extend the config** to register the tool, define its activation logic, and set relevant parameters (e.g., confidence thresholds, max outputs).

> 🎛️ This design gives you **complete control** over tool orchestration—**enabling lightweight or rich multimodal reasoning pipelines** depending on your use case.

---

## 🌟 Acknowledgments & License

* Built on top of **OpenAI’s CLIP**, **Whisper**, and **LLaVA** models—leveraging state-of-the-art architectures for multimodal understanding.
* Thanks to **Hugging Face** for maintaining an unparalleled model hub and community ecosystem.
* Licensed under the **MIT License**. Use it in academia or enterprise—modify, distribute, and innovate without restrictions. Contributions (Issues, Pull Requests) are highly welcome.
