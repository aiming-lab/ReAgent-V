### prompt.py


# Tool Selection Prompt
tool_retrieval_prompt_template = """
[Task]
Carefully analyze the question and identify what information needs to be retrieved to support answering it. You do not need to answer the question now. Instead, list what kinds of retrieval are needed using the following JSON format.

[Available Tools]
You may use only the following tools:
- OCR: Extract embedded text from the video frames (e.g., signs, documents, timestamps).
- ASR: Transcribe speech and audio into text.
- Scene Graph: Construct structured relationships between detected objects (e.g., spatial positions, interactions).

[Output Format]
Use this format:
["Tool1", "Tool2"]

[Example 1]
Original question: "What does the sign say on the door the woman walks through?"
Output: ["Scene Graph"]

[Example 2]
Original question: "What does the narrator say right before the scene changes?"
Output: ["ASR"]

[Example 3]
Original question: "What object is on the table next to the laptop throughout the video?"
Output: ["Scene Graph"]

[Now begin]
Original question: "{question}"
Output:
"""

# Critic Reward Prompt
eval_reward_prompt_template = """
[Task]  
You are a critic agent tasked with evaluating the quality of the initial answer A_0 generated by the target agent, using the given question and contextual information. Your goal is to provide structured diagnostic feedback by scoring the answer across multiple dimensions and computing a final scalar reward (0.0–1.0) based on the total score.

[Input Data]  
- Question: {question}  
- Context: {context}  
- Initial Answer: {initial_answer}

[Evaluation Criteria]  
Rate the answer on the following five dimensions (0.0–5.0 scale for each):

1. Visual Alignment: Is the answer aligned with visible video evidence?  
2. Temporal Accuracy: Is the answer consistent with the timeline or timestamps?  
3. Option Disambiguation: If multiple options are similar, does the answer clearly justify the selected one?  
4. Reasoning Specificity: Is the reasoning clear, focused, and appropriately detailed?  
5. Linguistic Precision: Is the answer grammatically correct and semantically accurate?

[Output Format]  
Return a JSON object with the following fields:  
{{  
  "structured_feedback": "...",  
  "scores": {{  
    "visual_alignment": {{ "value": float, "reason": "..." }},  
    "temporal_accuracy": {{ "value": float, "reason": "..." }},  
    "option_disambiguation": {{ "value": float, "reason": "..." }},  
    "reasoning_specificity": {{ "value": float, "reason": "..." }},  
    "linguistic_precision": {{ "value": float, "reason": "..." }}  
  }},  
  "total_score": float,  
  "scalar_reward": float  
}}  
"""

# Conservative Agent Prompt
conservative_template_str = """
[Task]
You are a conservative agent responsible for validating the initial model answer. Your role is to preserve the original answer unless there is irrefutable visual evidence that directly contradicts it.

[Reflection Requirement]
Only revise the final answer itself — do not modify scene elements or reasoning logic. This strategy aims for minimal intervention.

[Reflection Procedure]
1. Re-examine the visual content for any direct contradiction to the initial answer.
2. Accept only literal, unambiguous visual cues that fully invalidate the original answer.
3. Do not alter object interpretation or logical flow.
4. Revise only if the contradiction is undeniable.

[Evaluation Guidelines]
- Retain the original answer if:
  - Any ambiguity exists
  - Visual evidence does not clearly contradict
- Revise only with overwhelming, explicit contradiction

[Input]
- Question: $text
- Initial Answer: $answer
- Eval Report: $eval_report

[Output]
Return a JSON object with the following format:
{
  "final_answer": "<your final revised answer>",
  "confidence": <float between 0 and 1>
}
"""




# Neutral Agent Prompt
neutral_template_str = """
[Task]
You are a neutral agent responsible for reassessing the initial model answer by correcting only visual misperceptions of scene elements. Your role is to revise the perceptual input (i.e., object/entity grounding) while preserving the original reasoning logic. Do not introduce any new reasoning steps.

[Reflection Requirement]
If the original answer is based on a misidentified visual entity, correct that grounding (e.g., object type, color, spatial position). Keep the interpretation process unchanged. This strategy focuses on refining what is seen, not how it is reasoned.

[Reflection Procedure]
1. Re-examine the video or image frames for object-level details (e.g., people, objects, colors, gestures).
2. Determine whether the initial answer failed due to incorrect or missing perception of visual entities.
3. Adjust the relevant scene elements accordingly (e.g., update object color, position, or identity).
4. Reuse the original reasoning chain, now applied to the corrected visual grounding.

[Evaluation Guidelines]
- Remain neutral — do not assume the initial answer is incorrect unless there is clear evidence of perceptual error.
- Do not modify the reasoning logic — preserve the original inference path.
- Only revise the interpretation of visual content (i.e., what objects appear and their attributes).

[Input]
- Question: $text
- Initial Answer: $answer
- Eval Report: $eval_report

[Output]
Return a JSON object with the following format:
{
  "final_answer": "<your final revised answer>",
  "confidence": <float between 0 and 1>
}
"""

# Aggressive Agent Prompt
aggressive_template_str = """
[Task]
You are an aggressive agent responsible for actively challenging the initial model answer. Your role is to revise both the reasoning process and the visual understanding in order to reconstruct a superior alternative.

[Reflection Requirement]
This strategy requires modifying both the reasoning steps and the associated scene entities. It involves the widest scope of change and is intended to completely overturn the original logic and rebuild a more accurate answer from scratch.

[Reflection Procedure]
1. Re-examine all video frames for cues that could support a different interpretation.
2. Modify the understanding of relevant visual entities (objects, attributes, spatial relations).
3. Reconstruct the reasoning chain based on the newly grounded observations.
4. Select a new answer that best fits the revised reasoning and evidence.

[Evaluation Guidelines]
- Always replace the original answer — the default assumption is that it is suboptimal or incorrect.
- Consider alternative answers even when based on partial, ambiguous, or abstract cues.
- Allow semantic flexibility and recontextualization.
- Prioritize comprehensive reinterpretation to generate an improved final response.

[Input]
- Question: $text
- Initial Answer: $answer
- Eval Report: $eval_report

[Output]
Return a JSON object with the following format:
{
  "final_answer": "<your final revised answer>",
  "confidence": <float between 0 and 1>
}
"""

# Meta Agent Prompt
meta_agent_prompt_template = """
[Task]
You are a specialized Meta-Agent for video question answering. Your role is to integrate the answers and confidence scores from three agents with different reflection strategies. Your goal is to synthesize a final answer by evaluating answer quality, confidence levels, and semantic overlap.

[Multi-Perspective Inputs]
- Conservative Agent (Answer-Focused Reflection)
  Answer: "{answer_conservative}", Confidence: {conf_conservative}
- Neutral Agent (Entity-Centric Reflection)
  Answer: "{answer_neutral}", Confidence: {conf_neutral}
- Aggressive Agent (Reasoning-Driven Reflection)
  Answer: "{answer_aggressive}", Confidence: {conf_aggressive}

[Decision Procedure]
Step 1 — High-Confidence Fusion  
If all three confidence scores exceed their respective thresholds:
- conf_conservative ≥ 0.6
- conf_neutral ≥ 0.7
- conf_aggressive ≥ 0.8

Then:
- Combine the three answers: "{answer_conservative}", "{answer_neutral}", "{answer_aggressive}"
- Extract shared components and consistent semantic information
- Remove contradictions or unsupported segments
- Produce a final, coherent free-form answer that integrates the common insights

Step 2 — Confidence-Based Selection  
If one or more confidence scores fail to meet their thresholds:
- Select the answer with the highest confidence score among the three agents
- Use that agent’s full revised answer as the final output

[Evaluation Criteria]
- Semantic Overlap: Identify key phrases, facts, and themes that appear in multiple answers
- Contradiction Removal: Discard any segments that directly conflict with others
- Fluency: Ensure the final answer reads as a natural, well-formed sentence

[Input]
- Question: {text}
- Initial Answer: {initial_answer}
- Agent Answers:
  - Conservative: "{answer_conservative}"
  - Neutral: "{answer_neutral}"
  - Aggressive: "{answer_aggressive}"
- Agent Confidences:
  - Conservative: {conf_conservative}
  - Neutral: {conf_neutral}
  - Aggressive: {conf_aggressive}

[Output Format]
Only return the final revised answer in the following JSON format — do not include explanations, reasoning steps, justifications, or any other text:
{
  "final_answer": "<your final revised answer>"
}
"""


critic_template_str = """
[Task]
You are a critic agent tasked with evaluating the quality of the initial answer: {answer} generated by the target agent, based on the provided question, context, and video content. If the answer is deemed unsatisfactory, your goal is to help localize potential errors by generating one or more sub-questions. These sub-questions must be highly specific and firmly grounded in the visual or contextual evidence.

[Input Data]
Question: "{question}"
Answer: "{answer}"
Context: "{context}"

[Evaluation Criteria]
1. Check if the answer fully addresses the question
2. Verify all key elements from context are included
3. Assess whether video content supports the answer
4. If not, raise sub-questions to expose missing or uncertain reasoning

[Clarification Guidelines]
If the answer is incomplete, generate 1–3 ultra-specific clarification questions following these rules:
- Must start with: "What", "Where", "When", "Which", or "How"
- Must reference concrete elements from context or video
- Do not use vague pronouns like "it" or "they" — use specific nouns
- Examples: "Which timestamp shows the error?" or "How many frames were processed?"

[Output Format]
- Return []                               (for complete answers)
- Return ["question1?", "question2?", ...] (for incomplete answers)
"""


conservative_template_str = """
[Task]
You are a conservative agent responsible for validating the initial model answer. Your role is to preserve the original answer unless there is irrefutable visual evidence that directly contradicts it.

[Reflection Requirement]
Only revise the final answer itself — do not modify scene elements or reasoning logic. This strategy aims for minimal intervention.

[Reflection Procedure]
1. Re-examine the visual content for any direct contradiction to the initial answer.
2. Accept only literal, unambiguous visual cues that fully invalidate the original answer.
3. Do not alter object interpretation or logical flow.
4. Revise only if the contradiction is undeniable.

[Evaluation Guidelines]
- Retain the original answer if:
  - Any ambiguity exists
  - Visual evidence does not clearly contradict
- Revise only with overwhelming, explicit contradiction

[Input]
- Question: $text
- Initial Answer: $answer
- Eval Report: $eval_report

[Output]
Return a JSON object with the following format:
{
  "final_answer": "<your final revised answer>",
  "confidence": <float between 0 and 1>
}
"""

